"""

    This program scrapes the data from BCCL website.
    It makes use of the area_info.pickle file generated by
    running the scraping.py prorgam.

    It saves the final Data in the pickle format in the file
    "scraped_data.pickle" and in JSON format in file
    "scraped_data.json".

    See usage at the end of the program.
    
"""

import pickle
import requests
import time
import datetime
import json
import os
import random
from selenium import webdriver
import pymysql
from mongo_creds import *
from sshtunnel import SSHTunnelForwarder
import pymongo
from bs4 import BeautifulSoup

url = "http://www.bcclweb.in/reports/sales_order_details.php"

all_area_data = []


def scrape_all2(start_date, end_date):
    global all_area_data

    with open("area_info.pickle", "rb") as file:
        area_info = pickle.load(file)

    with open("scraped_data.pickle", "rb") as file:
        all_area_data = pickle.load(file)

    start_date = get_date_string(start_date)
    end_date = get_date_string(end_date)

    desired_data = []
    for area in list(area_info.keys()):
        area_id = area_info[area]['id']
        area_name = area
        collieries = area_info[area]['colliery_names']

        for (colliery_name, colliery_id) in collieries:
            print("scraping for {} in area {}".format(colliery_name, area_name))
            data = submit_values2(
                area_id, area_name, colliery_id, colliery_name, start_date, end_date)
            print(data)


def submit_values2(area_id, area_name, colliery_id, colliery_name, start_date, end_date):
    payload = "------WebKitFormBoundary7MA4YWxkTrZu0gW\r\nContent-Disposition: form-data; name=\"area_id\"\r\n\r\n{area_id}\r\n------WebKitFormBoundary7MA4YWxkTrZu0gW\r\nContent-Disposition: form-data; name=\"colliery_id\"\r\n\r\n{colliery_id}\r\n------WebKitFormBoundary7MA4YWxkTrZu0gW\r\nContent-Disposition: form-data; name=\"start_date\"\r\n\r\n{start_date}\r\n------WebKitFormBoundary7MA4YWxkTrZu0gW\r\nContent-Disposition: form-data; name=\"end_date\"\r\n\r\n{end_date}\r\n------WebKitFormBoundary7MA4YWxkTrZu0gW\r\nContent-Disposition: form-data; name=\"submit\"\r\n\r\n\r\n------WebKitFormBoundary7MA4YWxkTrZu0gW--".format(
        area_id=area_id, colliery_id=colliery_id, start_date=start_date, end_date=end_date)
    headers = {
        'content-type': "multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW",
        'cache-control': "no-cache",
        'Postman-Token': "20ee4f0c-1060-46fe-80e7-dcbc028e98a5"
    }

    response = requests.request("POST", url, data=payload, headers=headers)
    # print(response.text)
    return extract_with_bs4(response.text)



def scrape_past_dates(start_date="2019-01-03", end_date="2019-01-04"):

    start_date_obj = datetime.datetime.strptime(start_date, "%Y-%m-%d")
    end_date_obj = datetime.datetime.strptime(end_date, "%Y-%m-%d")

    # assert (end_date_obj - start_date_obj).days <= 7, "Interval of less than equal to seven days allowed"

    return scrape_all2(start_date_obj, end_date_obj)

def scrape_yesterday():
    start_date = datetime.datetime.now() - datetime.timedelta(1)
    end_date = start_date

    print("called the function")
    return scrape_all(start_date, end_date)

def get_date_string(date):
    format = "%Y-%m-%d"
    date_str = date.strftime(format)

    return date_str


def insert_data_mongo(data):

    print(data)

    server = SSHTunnelForwarder(
        MONGO_HOST,
        ssh_username=MONGO_USER,
        ssh_password=MONGO_PASS,
        remote_bind_address=('127.0.0.1', 27017)
    )
    print(server)
    server.start()
    print("yaha tak aa gaya")
    client = pymongo.MongoClient('127.0.0.1', server.local_bind_port)
    db = client[MONGO_DB]
    # print(db.collection_names())

    coll = db['sales_order_details']
    print(data)
    coll.insert(data)
    with open("log.txt", 'w') as file:
        file.write("Successful insertion")

    server.stop()

def extract_with_bs4(source_code):
    # source_code = driver.page_source
    soup = BeautifulSoup(source_code)

    rows = soup.findAll('tr')
    headings_row = rows[0]
    records_rows = rows[1:]

    headings = [heading.text for heading in headings_row.findAll('th')]

    data = []

    for record in records_rows:
        temp_dict = {}
        if len(record.findAll('td')) == 1:
            print("Found No Data")
            return data
        for index, cell in enumerate(record.findAll('td')):
            temp_dict[headings[index]] = cell.text
        data.append(temp_dict)
        # print(temp_dict)
        # break
    return data


"""
    Usage : 

        scrape_past_dates(start_date, end_date)
            Scrape the data from Starting Date to Ending Date

            Arguments:
                start_date : (String) Specify starting date in the format YYYY-MM-DD
                end_date   : (String) Specify end date in the format YYYY-MM-DD
            
            Example : scrape_past_dates(start_date = "2018-12-28", end_date = "2018-12-30")

        scrape_yesterday()
            Scrape the yesterday's data

            Arguments:
                None
            
            Example : scrape_yesterday()
"""
scraped_data = scrape_past_dates(
    start_date="2019-01-05", end_date="2019-01-06")
# scraped_data = scrape_yesterday()
insert_data_mongo(scraped_data)

with open("scraped_data.json", 'w') as file:
    json.dump(all_area_data, file)

driver.close()
